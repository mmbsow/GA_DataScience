{
 "metadata": {
  "name": "document_classification_2_solutions",
  "signature": "sha256:1d8d31aca2ac911bccac41b2c1c062d3c3edf22ea862850f08d95b68065533cd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Text Feature Dimensionality Reduction\n",
      "\n",
      "Recall that the *curse of dimensionality* refers to a collection of problems encountered when using high-dimensional data. A large corpus can easily contain hundreds of thousands of unique tokens. Let's examine some techniques for reducing the dimensions of text features.\n",
      "\n",
      "  \n",
      "#### Lower-casing\n",
      "\n",
      "A basic strategy for reducing the dimensions of the feature space is to convert all of the text to lowercase. This is motivated by the intuition that the letter case does not contribute to the meanings of most words: \"sandwich\" and \"Sandwich\" have the same meaning in most contexts. Capitalization may indicate that a word is beginning a sentence, but the bag-of-words model has already discarded all information from word order and grammar.\n",
      "\n",
      "  \n",
      "#### Stop Word Filtering\n",
      "\n",
      "A second strategy is to remove words that are common to most of the documents in the corpus. These words, called stop words, frequently include determiners like \"the\" and \"an\", auxiliary verbs like \"do\" and \"has\", and prepositions, like \"on\" and \"beneath\". Stop words are often functional words that contribute to the document's meaning through grammar rather than their denotations. \n",
      "\n",
      "CountVectorizer can filter stop words provided as the `stop_words` keyword parameter, and also includes a basic English stop list. Let's re-create the feature vectors for our documents using lower-casing and stop filtering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import CountVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "corpus = [\n",
      "    'The sandwich was eaten by me.',\n",
      "    'I ate a sandwich, you ate a SANDWICH, and the cat ate a sandwich.'\n",
      "]\n",
      "# TODO Create an instance of `CountVectorizer`. Set the `stop_words` keyword argument to 'english'.\n",
      "tf_vectorizer = CountVectorizer(stop_words='english')\n",
      "# TODO fit and transform the corpus.\n",
      "X_toy = tf_vectorizer.fit_transform(corpus)\n",
      "print tf_vectorizer.vocabulary_\n",
      "# TODO print the features and the vocabulary. What happened?\n",
      "print X_toy.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'sandwich': 3, u'ate': 0, u'eaten': 2, u'cat': 1}\n",
        "[[0 0 1 1]\n",
        " [3 1 0 3]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Stemming and Lemmatization\n",
      "\n",
      "While stop filtering is an easy strategy for dimensionality reduction, most stop lists contain only a few hundred words. A large corpus may still have hundreds of thousands of unique words after filtering. Two similar strategies for reducing dimensionality are called stemming and lemmatization.  \n",
      "\n",
      "A high-dimensional document vector may separately encode several derived or inflected forms of the same word. For example, \"jumping\" and \"jumps\" are both forms of the word \"jump\"; a document vector in a corpus of long-jumping articles may encode each inflected form with a separate element in the feature vector.  Stemming and lemmatization are two strategies for condensing inflected and derived forms of a word into a single feature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Consider the following corpus:\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "corpus = [\n",
      "    'He ate the sandwiches',\n",
      "    'Every sandwich was eaten by him'\n",
      "]\n",
      "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
      "print vectorizer.fit_transform(corpus).todense()\n",
      "print vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 0 1]\n",
        " [0 1 1 0]]\n",
        "{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The documents have similar meanings, but their feature vectors have no elements in common! Both documents contain a conjugation of \u201cate\u201d and an inflected form of \u201csandwich\u201d; ideally, these similarities should be reflected in the feature vectors.  \n",
      "\n",
      "*Lemmatization* is the process of determining the lemma, or the morphological root of an inflected word based on its context. Lemmas are the base forms of words that index the word in the dictionary.  \n",
      "\n",
      "*Stemming* has a similar goal to lemmatization, but it does not attempt to produce the morphological roots of words. Instead, stemming removes all patterns of characters that appear to be affixes, resulting in a token that is not necessarily a valid word. Lemmatization frequently requires a lexical resource, like WordNet, and the word's part-of-speech. Stemming algorithms frequently use rules instead of lexical resources to produce stems, and can operate on any token, even without its context."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO if you do not have NLTK, you can install it using `pip install nltk`.\n",
      "# TODO import the functions `word_tokenize` and `pos_tag` from the module `nltk`.\n",
      "from nltk import word_tokenize, pos_tag\n",
      "# TODO import the class `PorterStemmer` from NLTK's `stem` module.\n",
      "from nltk.stem import PorterStemmer\n",
      "# TODO import the class `WordNetLemmatizer` from `nltk.stem.wordnet`.\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "\n",
      "wordnet_tags = ['n', 'v']\n",
      "corpus = [\n",
      "    'He ate the sandwiches',\n",
      "    'Every sandwich was eaten by him'\n",
      "]\n",
      "\n",
      "# TODO instantiate a `PorterStemmer`\n",
      "# stemmer = \n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "# TODO use `word_tokenize` to token each document in the corpus. Then stem and print each token.\n",
      "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]\n",
      "\n",
      "# TODO instantiate a WordNetLemmatizer.\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "# The WordNetLemmatizer requires the token and its corresponding part-of-speech.\n",
      "# For a complete list of the Penn Treebank POS tags see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
      "# The following function takes a token and its POS tag and returns the lemma or None.\n",
      "def lemmatize(token, tag):\n",
      "    if tag[0].lower() in ['n', 'v']:\n",
      "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
      "    return token\n",
      "\n",
      "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
      "print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
        "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Improving the Bag-of-Words Representation\n",
      "\n",
      "### Term Frequency Weights\n",
      "\n",
      "So far we have represented whether or not the token is present in the document. It is intuitive that the frequency with which a token appears in a text is indicative of \"how much\" the document pertains to that word. A document that contains a word once may be about a different topic than a document that contains the same word dozens of times. Let's represent our documents using raw term frequencies instead of binary frequencies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's examine the term frequencies produced for a toy corpus.\n",
      "corpus = [\n",
      "    'I ate some pizza.',\n",
      "    'I ate a sandwich, you ate a sandwich, and the cat ate a sandwich.'\n",
      "]\n",
      "# TODO create a new instance of CountVectorizer. Do not pass the keyword argument `binary=True`. \n",
      "# It will default to false, and return raw term frequencies.\n",
      "# tf_vectorizer =\n",
      "tf_vectorizer = CountVectorizer(stop_words='english')\n",
      "# TODO fit `tf_vectorizer` on the corpus, and transform the corpus\n",
      "# X_toy = \n",
      "X_toy = tf_vectorizer.fit_transform(corpus)\n",
      "# TODO print `tf_vectorizer`'s vocabulary and the transformed features.\n",
      "print X_toy.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 1 0]\n",
        " [3 1 0 3]]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Normalized Term Frequency Weights\n",
      "\n",
      "Encoding the terms' raw frequencies in the feature vector provides additional information about the meanings of the documents, but assumes that all of the documents are of similar lengths. Many words might appear with the same frequency in two documents, but the documents could still be dissimilar if one document is many times larger than the other.  \n",
      "\n",
      "scikit-learn's `TfdfTransformer` can mitigate this problem by transforming a matrix of term frequency weights into a matrix of normalized term frequency weights. By default, `TfdfTransformer` smooths the raw counts and applies L2 normalization. The smoothed, normalized term frequencies are given by the equation:\n",
      "\n",
      "$tf(t, f) = \\frac{f(t, d)}{||x||}$\n",
      "\n",
      "### Inverse Document Weights\n",
      "\n",
      "While normalized term frequencies mitigate the effects of different document lengths, another problem remains. Our feature vectors contain large weights for terms that occur frequently in a document, even if those terms occur frequently in most documents in the corpus. These terms do not help to represent the meaning of a particular document relative to the rest of the corpus. For example, most of the documents in a corpus of articles about Duke's basketball team could include the words \u201cbasketball\u201d, \u201cCoach K\u201d, and \u201cflop\u201d. These words can be thought of as corpus-specific stop words, and may not be useful for calculating the similarity of documents. The *inverse document frequency*, or IDF, is a measure of how rare or common a word is in a corpus. The inverse document frequency is given by the equation:\n",
      "\n",
      "$idf(t, d) = log \\frac{N}{1 + | d \\in D: t \\in d|}$\n",
      "\n",
      "where $N$ is the total number of documents in the corpus, and $| d \\in D: t \\in d|$ is the number of documents in the corpus that contain term . A term's tf-idf weight is the product of its term frequency and inverse document frequency. `TfidfTransformer` returns tf-idfs weight when its `use_idf` keyword argument is set to its default value, `True`. Since tf-idf weighted feature vectors are commonly used to represent text, scikit-learn provides a `TfidfVectorizer` class that wraps `CountVectorizer` and `TfidfTransformer`. Let's use `TfidfVectorizer` to create tf-idf weighted feature vectors for our corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the fetch_20newsgroups function from data sets.\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "# TODO import the Pipeline class\n",
      "from sklearn.pipeline import Pipeline\n",
      "# TODO import TfidfVectorize\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# TODO import Perceptron\n",
      "from sklearn.linear_model import Perceptron\n",
      "# TODO import cross_val_score\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "# We will start with two categories\n",
      "categories = ['alt.atheism', 'rec.sport.hockey']\n",
      "# We will load the documents for these categories. We will remove the header and footer fields from the\n",
      "# documents since these are highly indicative of the categories.\n",
      "newsgroups = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))\n",
      "\n",
      "# TODO assign the data and labels\n",
      "# X_raw = \n",
      "# y_raw = \n",
      "X_raw = newsgroups.data\n",
      "y_raw = newsgroups.target\n",
      "print len(X_raw), len(y_raw)\n",
      "\n",
      "# TODO create a Pipeline containing a TfidfVectorizer and a Perceptron\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', TfidfVectorizer(binary=False, use_idf=True, stop_words='english')),\n",
      "                     ('clf', Perceptron())\n",
      "])\n",
      "# TODO evaluate the accuracy, precision, and recall using cross validation.\n",
      "# accuracy_scores = \n",
      "# precision_scores = \n",
      "# recall_scores = \n",
      "# TODO print the scores and their means. Did using term frequency weights help?\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print accuracy_scores, np.mean(accuracy_scores)\n",
      "print precision_scores, np.mean(precision_scores)\n",
      "print recall_scores, np.mean(recall_scores)\n",
      "\n",
      "# TODO now compare the performance of the model using binary term frequencies, TF weights, and TF-IDF weights.\n",
      "# Grid search to optimize the hyperparameters of TfidfVectorizer.\n",
      "# Which features performed best?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1080 1080\n",
        "[ 0.9212963   0.86574074  0.90740741  0.93981481  0.90740741]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.908333333333\n",
        "[ 0.91869919  0.85271318  0.90322581  0.98198198  0.90983607] 0.913291243859\n",
        "[ 0.94166667  0.91666667  0.93333333  0.90833333  0.925     ] 0.925\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
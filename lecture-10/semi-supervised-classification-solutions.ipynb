{
 "metadata": {
  "name": "semi-supervised-classification-solutions"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clustering Features\n",
      "\n",
      "In this lab we build a semi-supervised classifier that can predict whether a photograph depicts a cat or a dog.  \n",
      "\n",
      "![files/cats-and-dogs-img/cat.94.jpg](files/cats-and-dogs-img/cat.9364.jpg)\n",
      "![files/cats-and-dogs-img/dog.8892.jpg](files/cats-and-dogs-img/dog.8892.jpg)\n",
      "\n",
      "Classifying natural images is a challenging problem. We will extract a variable number of SURF descriptors from each image.  \n",
      "Describing SURF descriptors is beyond the scope of this class; for this lab it is sufficient to know that they describe the \"interesting\" parts of an image.\n",
      "We then cluster the SURF descriptors, and train the classifier using the clusters as features.  \n",
      "This approach is sometimes called the \"bag-of-features\" representation, since it is analogous to the bag-of-words representation.  \n",
      "Note that this approach does not achieve state-of-the-art performance in object recognition, but it is a good demonstration of using clustering for vector quantization, and state-of-the-art approaches may too costly for some problems.  \n",
      "\n",
      "We require the `mahotas` library to cluster the images. Install `mahotas` by executing `pip install mahotas` in your terminal emulator.  \n",
      "\n",
      "Please execute the next two cells at the beginning of class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First we will make a list of the image files that need to be loaded and create a list of the classes.\n",
      "import glob\n",
      "import random\n",
      "all_instance_filenames = []\n",
      "all_instance_targets = []\n",
      "for f in glob.glob('cats-and-dogs-img/*.jpg'):\n",
      "    target = 1 if 'cat' in f[18:] else 0\n",
      "    all_instance_filenames.append(f)\n",
      "    all_instance_targets.append(target)\n",
      "    \n",
      "combined = zip(all_instance_filenames, all_instance_targets)\n",
      "random.shuffle(combined)\n",
      "all_instance_filenames[:], all_instance_targets[:] = zip(*combined)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We will extract SURF descriptors from each of the images.\n",
      "import mahotas as mh\n",
      "from mahotas.features import surf\n",
      "surf_features = []\n",
      "counter = 0\n",
      "for f in all_instance_filenames:\n",
      "    if counter % 100 == 0:\n",
      "        print 'Loaded image %s of 2000' % counter\n",
      "    counter += 1\n",
      "    image = mh.imread(f, as_grey=True)\n",
      "    surf_features.append(surf.surf(image)[:, 5:])\n",
      "print 'SURF extraction complete.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded image 0 of 2000\n",
        "Loaded image 100 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 200 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 300 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 400 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 500 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 600 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 700 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 800 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 900 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1000 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1100 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1200 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1300 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1400 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1500 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1600 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1700 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1800 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1900 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SURF extraction complete."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO how many images did we load?\n",
      "len(surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "1999"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO would you expect the same number of SURF descriptors to be extracted from each image?\n",
      "# TODO check your answer by printing the shapes of a few of the elements in surf_features.\n",
      "print surf_features[0].shape\n",
      "print surf_features[1].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(859, 65)\n",
        "(819, 65)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import numpy, LogisticRegression, MiniBatchKMeans, and the entire metrics module.\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import *\n",
      "from sklearn.cluster import MiniBatchKMeans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:3: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._min_spanning_tree import minimum_spanning_tree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:3: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._min_spanning_tree import minimum_spanning_tree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/_graph_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\\\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/_graph_validation.py:5: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\\\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._traversal import connected_components\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:4: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._traversal import connected_components\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py:20: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .sparsefuncs_fast import csr_row_norms\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.py:22: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .sparsefuncs_fast import csr_row_norms\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .ckdtree import *\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .ckdtree import *\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .qhull import *\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .qhull import *\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/stats/distributions.py:35: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import vonmises_cython\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/scipy/stats/distributions.py:35: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import vonmises_cython\n",
        "/usr/local/lib/python2.7/dist-packages/scipy/stats/stats.py:252: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._rank import rankdata, tiecorrect\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/scipy/stats/stats.py:252: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._rank import rankdata, tiecorrect\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .expected_mutual_info_fast import expected_mutual_information\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .expected_mutual_info_fast import expected_mutual_information\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import cd_fast\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import cd_fast\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import libsvm, liblinear\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import libsvm, liblinear\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import libsvm_sparse\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import libsvm_sparse\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._random import sample_without_replacement\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._random import sample_without_replacement\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/__init__.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .ball_tree import BallTree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/__init__.py:6: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .ball_tree import BallTree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/__init__.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .kd_tree import KDTree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/__init__.py:7: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .kd_tree import KDTree\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/graph.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from .graph_shortest_path import graph_shortest_path\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/utils/graph.py:16: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from .graph_shortest_path import graph_shortest_path\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/isotonic.py:11: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from ._isotonic import _isotonic_regression\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/isotonic.py:11: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from ._isotonic import _isotonic_regression\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/manifold/t_sne.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import _utils\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/manifold/t_sne.py:21: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import _utils\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py:34: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import _k_means\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py:34: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import _k_means\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/hierarchical.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
        "  from . import _hierarchical\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/hierarchical.py:24: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility\n",
        "  from . import _hierarchical\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now we are going to split the data into training and test sets. \n",
      "# We cannot use train_test_split because X is not a design matrix; a different number of SURF descriptors were extracted for each image.\n",
      "# TODO set train_len equal to 75% of the instances\n",
      "train_len = int(len(all_instance_filenames) * .75)\n",
      "# TODO Use NumPy to concatenate all of the rows of the training set into one array. We will cluster these features.\n",
      "X_train_surf_features = np.concatenate(surf_features[:train_len])\n",
      "# TODO set y_train equal to the classes for the training instances\n",
      "y_train = all_instance_targets[:train_len]\n",
      "# TODO set y_test equal to the classes for the testing instances\n",
      "y_test = all_instance_targets[train_len:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_clusters = 150\n",
      "print 'Clustering', len(X_train_surf_features), 'features'\n",
      "# TODO instantiate MiniBatchKMeans. Set n_clusters.\n",
      "estimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
      "# TODO fit the estimator on the training features.\n",
      "estimator.fit_transform(X_train_surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Clustering 946184 features\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([[ 4.76476233,  1.73791798,  3.02333796, ...,  4.04065997,\n",
        "         0.62782508,  3.96105146],\n",
        "       [ 4.84879765,  1.79616283,  3.12647702, ...,  4.11503782,\n",
        "         0.59234906,  4.05704974],\n",
        "       [ 2.00121136,  1.28918883,  0.68553205, ...,  1.33657523,\n",
        "         2.56947634,  1.27816843],\n",
        "       ..., \n",
        "       [ 2.42608001,  1.10905144,  0.87719747, ...,  1.54950691,\n",
        "         2.29401348,  1.62437218],\n",
        "       [ 2.98182304,  1.03660259,  1.45612098, ...,  2.31054276,\n",
        "         1.90829252,  2.23174201],\n",
        "       [ 2.56454424,  1.32649726,  1.17536557, ...,  1.80595575,\n",
        "         2.39123943,  1.82664612]])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We have now clustered the SURF descriptors for the test set.\n",
      "# We can now create consistent feature representations for the training set.\n",
      "# For each training image, we will assign each of its extracted SURF descriptors to one of the 150 clusters.\n",
      "# TODO how many features will our design matrix have?\n",
      "X_train = []\n",
      "for instance in surf_features[:train_len]:\n",
      "    if len(instance) == 0:\n",
      "        X_train.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_train.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we will assign each of the SURF descriptors for each of the testing instances to one of the training clusters.\n",
      "X_test = []\n",
      "for instance in surf_features[train_len:]:\n",
      "    if len(instance) == 0:\n",
      "        X_test.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_test.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO print the dimensions of X_train and X_test\n",
      "print type(X_train)\n",
      "print len(X_train)\n",
      "print X_train[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n",
        "1499\n",
        "(150,)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO instantiate a LogisticRegression classifier\n",
      "clf = LogisticRegression()\n",
      "# TODO fit the classifier.\n",
      "clf.fit_transform(X_train, y_train)\n",
      "# TODO evaluate the classifier's accuracy, precision and recall.\n",
      "# TODO print the classification report for the classifier.\n",
      "predictions = clf.predict(X_test)\n",
      "print classification_report(y_test, predictions)\n",
      "print 'Precision:', precision_score(y_test, predictions)\n",
      "print 'Recall:', recall_score(y_test, predictions)\n",
      "print 'Accuracy:', accuracy_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.71      0.64      0.67       256\n",
        "          1       0.66      0.73      0.69       244\n",
        "\n",
        "avg / total       0.68      0.68      0.68       500\n",
        "\n",
        "Precision: 0.657992565056\n",
        "Recall: 0.725409836066\n",
        "Accuracy: 0.682\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO fit another classifier on the training data. Does it perform better?\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "clf2 = KNeighborsClassifier(n_neighbors=30)\n",
      "clf2.fit(X_train, y_train)\n",
      "print clf2.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.67\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO grid search to optimize the hyperparameters of one of the classifiers."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO does changing the number of clusters improve performance?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
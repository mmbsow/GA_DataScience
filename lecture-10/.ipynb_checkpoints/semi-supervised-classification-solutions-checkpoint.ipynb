{
 "metadata": {
  "name": "semi-supervised-classification-solutions"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clustering Features\n",
      "\n",
      "In this lab we build a semi-supervised classifier that can predict whether a photograph depicts a cat or a dog.  \n",
      "\n",
      "![files/cats-and-dogs-img/cat.94.jpg](files/cats-and-dogs-img/cat.9364.jpg)\n",
      "![files/cats-and-dogs-img/dog.8892.jpg](files/cats-and-dogs-img/dog.8892.jpg)\n",
      "\n",
      "Classifying natural images is a challenging problem. We will extract a variable number of SURF descriptors from each image.  \n",
      "Describing SURF descriptors is beyond the scope of this class; for this lab it is sufficient to know that they describe the \"interesting\" parts of an image.\n",
      "We then cluster the SURF descriptors, and train the classifier using the clusters as features.  \n",
      "This approach is sometimes called the \"bag-of-features\" representation, since it is analogous to the bag-of-words representation.  \n",
      "Note that this approach does not achieve state-of-the-art performance in object recognition, but it is a good demonstration of using clustering for vector quantization, and state-of-the-art approaches may too costly for some problems.  \n",
      "\n",
      "We require the `mahotas` library to cluster the images. Install `mahotas` by executing `pip install mahotas` in your terminal emulator.  \n",
      "\n",
      "Please execute the next two cells at the beginning of class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First we will make a list of the image files that need to be loaded and create a list of the classes.\n",
      "import glob\n",
      "all_instance_filenames = []\n",
      "all_instance_targets = []\n",
      "for f in glob.glob('cats-and-dogs-img/*.jpg'):\n",
      "    target = 1 if 'cat' in f[18:] else 0\n",
      "    all_instance_filenames.append(f)\n",
      "    all_instance_targets.append(target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We will extract SURF descriptors from each of the images.\n",
      "import mahotas as mh\n",
      "from mahotas.features import surf\n",
      "surf_features = []\n",
      "counter = 0\n",
      "for f in all_instance_filenames:\n",
      "    if counter % 100 == 0:\n",
      "        print 'Loaded image %s of 2000' % counter\n",
      "    counter += 1\n",
      "    image = mh.imread(f, as_grey=True)\n",
      "    surf_features.append(surf.surf(image)[:, 5:])\n",
      "print 'SURF extraction complete.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO how many images did we load?\n",
      "len(surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO would you expect the same number of SURF descriptors to be extracted from each image?\n",
      "# TODO check your answer by printing the shapes of a few of the elements in surf_features.\n",
      "print surf_features[0].shape\n",
      "print surf_features[1].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import numpy, LogisticRegression, MiniBatchKMeans, and the entire metrics module.\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import *\n",
      "from sklearn.cluster import MiniBatchKMeans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now we are going to split the data into training and test sets. \n",
      "# We cannot use train_test_split because X is not a design matrix; a different number of SURF descriptors were extracted for each image.\n",
      "# TODO set train_len equal to 75% of the instances\n",
      "train_len = int(len(all_instance_filenames) * .75)\n",
      "# TODO Use NumPy to concatenate all of the rows of the training set into one array. We will cluster these features.\n",
      "X_train_surf_features = np.concatenate(surf_features[:train_len])\n",
      "# TODO set y_train equal to the classes for the training instances\n",
      "y_train = all_instance_targets[:train_len]\n",
      "# TODO set y_test equal to the classes for the testing instances\n",
      "y_test = all_instance_targets[train_len:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_clusters = 150\n",
      "print 'Clustering', len(X_train_surf_features), 'features'\n",
      "# TODO instantiate MiniBatchKMeans. Set n_clusters.\n",
      "estimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
      "# TODO fit the estimator on the training features.\n",
      "estimator.fit_transform(X_train_surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We have now clustered the SURF descriptors for the test set.\n",
      "# We can now create consistent feature representations for the training set.\n",
      "# For each training image, we will assign each of its extracted SURF descriptors to one of the 150 clusters.\n",
      "# TODO how many features will our design matrix have?\n",
      "X_train = []\n",
      "for instance in surf_features[:train_len]:\n",
      "    if len(instance) == 0:\n",
      "        X_train.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_train.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we will assign each of the SURF descriptors for each of the testing instances to one of the training clusters.\n",
      "X_test = []\n",
      "for instance in surf_features[train_len:]:\n",
      "    if len(instance) == 0:\n",
      "        X_test.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_test.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO print the dimensions of X_train and X_test\n",
      "print type(X_train)\n",
      "print len(X_train)\n",
      "print X_train[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO instantiate a LogisticRegression classifier\n",
      "clf = LogisticRegression()\n",
      "# TODO fit the classifier.\n",
      "clf.fit_transform(X_train, y_train)\n",
      "# TODO evaluate the classifier's accuracy, precision and recall.\n",
      "# TODO print the classification report for the classifier.\n",
      "predictions = clf.predict(X_test)\n",
      "print classification_report(y_test, predictions)\n",
      "print 'Precision:', precision_score(y_test, predictions)\n",
      "print 'Recall:', recall_score(y_test, predictions)\n",
      "print 'Accuracy:', accuracy_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO fit another classifier on the training data. Does it perform better?\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "clf2 = KNeighborsClassifier(n_neighbors=30)\n",
      "clf2.fit(X_train, y_train)\n",
      "print clf2.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO grid search to optimize the hyperparameters of one of the classifiers."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BONUS TODO does changing the number of clusters improve performance?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
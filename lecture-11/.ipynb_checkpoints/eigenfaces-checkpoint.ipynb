{
 "metadata": {
  "name": "eigenfaces"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Face Recognition with Eigenfaces\n",
      "\n",
      "Now let's apply PCA to a face recognition problem. Face recognition is the supervised classification task of identifying a person from an image of his or her face. In this example we will use a data set called Our Database of Faces from ATT Laboratories Cambridge. The data set contains ten images of each of forty people. The images were created under different lighting conditions, and the subjects varied their facial expressions. The images are gray-scale and 92x112 pixels in dimension. Let's look at some of the images.  \n",
      "\n",
      "While these images are small, a feature vector that encodes the intensity of every pixel will have 10,304 dimensions. Training from such high-dimensional data could require many samples to avoid over-fitting and will be computationally expensive at large scales. Instead, we will use PCA to compactly represent the images in terms of a small number of principal components. These compressed representations will be much smaller at the expense of discarding some information. We will not discard information randomly; we will keep synthetic features that explain as much of the variance as possible. For faces, that means we will keep enough information to create _fuzzy ghost faces_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![http://upload.wikimedia.org/wikipedia/commons/6/67/Eigenfaces.png](http://upload.wikimedia.org/wikipedia/commons/6/67/Eigenfaces.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can reshape the matrix of pixel intensities for an image into a vector, and create a matrix of these vectors for all of the training images. Each image is a linear combination of this data set's principal components. In the context of face recognition, these principal components are called eigenfaces. The eigenfaces can be thought of as standardized components of faces. Each face in the data set can be expressed as some combination of the eigenfaces, and can be approximated as a combination of the most important eigenfaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import numpy\n",
      "\n",
      "# TODO import mahotas\n",
      "\n",
      "# TODO import os\n",
      "\n",
      "# TODO import walk form os\n",
      "\n",
      "# TODO import the scale function from the preprocessing module\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loading the data is a bit hairy due to the directory structure and file names.\n",
      "X = []\n",
      "y = []\n",
      "for dirpath, dirnames, filenames in walk('data/att-faces/orl_faces'):\n",
      "    for fn in filenames:\n",
      "        if fn[-3:] == 'pgm':\n",
      "            # Get the path to the file\n",
      "            image_filename = os.path.join(dirpath, fn)\n",
      "            # Load the image, convert it to grayscale, and reshape the pixmap/matrix/grid to a vector.\n",
      "            X.append(scale(mh.imread(image_filename, as_grey=True).reshape(10304).astype('float32')))\n",
      "            y.append(dirpath)\n",
      "\n",
      "X = np.array(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the train_test_split convenience function\n",
      "\n",
      "# TODO split the data into training and test sets\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import PCA from the decomposition module\n",
      "\n",
      "# TODO instantiate a PCA estimator that will return the first 150 principal components\n",
      "\n",
      "# TODO fit the estimator on the training data\n",
      "# TODO transform the training data\n",
      "\n",
      "# TODO transform the test data\n",
      "\n",
      "# TODO print the shapes of the reduced data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import LogisticRegression from the linear_model module\n",
      "\n",
      "# TODO import classification_report from the metrics module\n",
      "\n",
      "# TODO instantiate a LogisticRegression classifier\n",
      "\n",
      "# TOOD fit the classifier on the reduced training data\n",
      "\n",
      "# TODO make predictions for the test set\n",
      "\n",
      "# TODO print the classification report for the test set\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO print some predictions and the corresponding true labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}
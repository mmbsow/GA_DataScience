{
 "metadata": {
  "name": "",
  "signature": "sha256:88e66539250aba7a72c8633032497a837215fc448dbfb29e9a63c33e3dd3dc7b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentiment Analysis on Movie Reviews by Rotten Tomatoes\n",
      "\n",
      "(https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n",
      "\n",
      "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. Each Sentence has been parsed into many phrases by the Stanford parser. Each row represents a phrase. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.  \n",
      "\n",
      "The sentiment labels are:  \n",
      "0 - negative  \n",
      "1 - somewhat negative  \n",
      "2 - neutral  \n",
      "3 - somewhat positive  \n",
      "4 - positive"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import required librairies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import PorterStemmer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "from scipy.sparse import hstack\n",
      "from scipy import sparse\n",
      "\n",
      "import datetime\n",
      "import math\n",
      "import re\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Tools functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function is used to compute the time taken to run a given cell"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def timestamp(date):\n",
      "    return time.mktime(date.timetuple())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function converts a phrase to an English unicode words separated by single space"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(phrase):\n",
      "  return \" \".join(re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Load the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the train data\n",
      "train = pd.read_table(\"train.tsv\")\n",
      "#train = train[train['SentenceId'] < 1500] # subset for testing purposes, limit to 1500 sentences\n",
      "train.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Phrase</th>\n",
        "      <th>Sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                          A series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                                 A</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                            series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "   PhraseId  SentenceId                                             Phrase  \\\n",
        "0         1           1  A series of escapades demonstrating the adage ...   \n",
        "1         2           1  A series of escapades demonstrating the adage ...   \n",
        "2         3           1                                           A series   \n",
        "3         4           1                                                  A   \n",
        "4         5           1                                             series   \n",
        "\n",
        "   Sentiment  \n",
        "0          1  \n",
        "1          2  \n",
        "2          2  \n",
        "3          2  \n",
        "4          2  "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Load the embeddings-scaled word vectors\n",
      "(http://metaoptimize.com/projects/wordreprs/)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "embed_df = pd.read_table(\"embeddings-scaled.EMBEDDING_SIZE=50.txt\", header=None, sep=' ', names=['word'] + range(1,51))\n",
      "embeddings = {row[0]: list(row[1:]) for row in embed_df.as_matrix()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "afinn_df = pd.read_table(\"AFINN-111.txt\",header=None, names=['word','score'])\n",
      "afinns = {row[0]: row[1] for row in afinn_df.as_matrix()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As pre-processing step, for each phrase in the train set, generate a vector containing the following information for a phrase:\n",
      "- the count of words;  \n",
      "- the count of punctuation signs counts;  \n",
      "- a words vector representing the aggregate (element-wise) of the vectors for the individual words in the phrase."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "all_phrase_embeds = {}\n",
      "all_phrase_afinns = {}\n",
      "\n",
      "for index, row in train.iterrows():\n",
      "    # build the list of actual words, and remove stop words\n",
      "    words_list = re.findall(r\"[\\w]+\", row['Phrase'], flags = re.UNICODE | re.LOCALE)\n",
      "    embeds_v = [ [0 for i in range(1, 51)] ]\n",
      "    embeds_v += [ embeddings.get(word, [0 for i in range(1,51)]) for word in words_list ]\n",
      "    afinns_v = [ afinns.get(word, 0) for word in words_list ]\n",
      "    \n",
      "    all_phrase_embeds[row['PhraseId']] = embeds_v\n",
      "    all_phrase_afinns[row['PhraseId']] = afinns_v\n",
      "    \n",
      "endtime = datetime.datetime.now()\n",
      "print 'Run time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Run time (secs):  26.0\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(all_phrase_embeds)\n",
      "print len(all_phrase_afinns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "156060\n",
        "156060\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Split the train set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split into X and y vectors\n",
      "X = train[['PhraseId','Phrase']]\n",
      "y = train['Sentiment']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'X_train: ', X_train.shape, '\\ty_train: ', y_train.shape\n",
      "print 'X_test: ', X_test.shape, '\\ty_test: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X_train:  (117045, 2) \ty_train:  (117045,)\n",
        "X_test:  (39015, 2) \ty_test:  (39015,)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Transformer Classes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create class that will tokenize and stem the corpus  \n",
      "*(not used: accuracy score is better when stemmer is not used)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class StemTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
      "        \n",
      "    def __call__(self, document):\n",
      "        return [ self.stemmer.stem(token) for token in self.tokenizer.tokenize(document) ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create class that will vectorize a corpus using our StemTokenizer and word vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SentAnaVectorizer(CountVectorizer):\n",
      "    def __init__(self, binary=False, strip_accents=None, stop_words=u'english', analyzer=u'word', \n",
      "                 ngram_range=(1, 1), lowercase=True, max_df=0.5, min_df=1, max_features=None,\n",
      "                 vocabulary=None, tokenizer=None, use_embeddings=False, use_afinns=True, aggregate=sum):\n",
      "        super(SentAnaVectorizer, self).__init__(self,\n",
      "                                                binary=binary, strip_accents=strip_accents, stop_words=stop_words, analyzer=analyzer,\n",
      "                                                ngram_range=ngram_range, lowercase=lowercase, max_df=max_df, min_df=min_df, \n",
      "                                                max_features=max_features, vocabulary=vocabulary, tokenizer=tokenizer)\n",
      "        self.use_embeddings = use_embeddings\n",
      "        self.use_afinns = use_afinns\n",
      "        self.aggregate = sum if not aggregate else aggregate\n",
      "    \n",
      "    def _phrase2vector(self, phraseId, phrase):\n",
      "        '''\n",
      "        Returns a vector containing the following information for a phrase:\n",
      "        - the count of words;\n",
      "        - the count of punctuation signs counts;\n",
      "        - a words vector representing the aggregate (element-wise) of the vectors for the individual words in the phrase.\n",
      "        '''        \n",
      "        # build the list of actual words, and remove stop words\n",
      "        words_list = re.findall(r\"[\\w]+\", phrase, flags = re.UNICODE | re.LOCALE)\n",
      "        words_list = [ w for w in words_list if w not in self.stop_words_ ]\n",
      "        \n",
      "        # build the list of punctuation signs\n",
      "        punct_list = re.findall(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~]\", phrase)\n",
      "        \n",
      "        # first add the count of words in the phrase and the count of punctuation signs\n",
      "        return_vector = [ len(words_list), len(punct_list) ]\n",
      "        \n",
      "        if self.use_afinns:\n",
      "            # if embeddings scaled word vectors provided, add to features\n",
      "            word_afinns = all_phrase_afinns.get(phraseId, [])\n",
      "\n",
      "            # now aggregate all the word vectors and add to the phrase vector\n",
      "            return_vector += [ self.aggregate(word_afinns) ]\n",
      "        \n",
      "        if self.use_embeddings:\n",
      "            # if embeddings scaled word vectors provided, add to features\n",
      "            word_embeds = all_phrase_embeds.get(phraseId, [[0 for i in range(1, 51)]])\n",
      "\n",
      "            # now aggregate all the word vectors and add to the phrase vector\n",
      "            return_vector += [ self.aggregate(x) for x in zip(*word_embeds) ]\n",
      "        \n",
      "        return return_vector\n",
      "\n",
      "    def fit(self, corpus, labels=None):\n",
      "        '''\n",
      "        Fits the corpus\n",
      "        '''\n",
      "        clean_corpus = corpus\n",
      "        if clean_corpus.ndim > 1:\n",
      "            clean_corpus = map(lambda phrase: clean(phrase), corpus[:,1]) # second column of corpus (Phrase)\n",
      "        return super(SentAnaVectorizer, self).fit(clean_corpus) \n",
      "        \n",
      "    def transform(self, corpus):\n",
      "        '''\n",
      "        Transforms the corpus\n",
      "        '''\n",
      "        # transform using super class\n",
      "        clean_corpus = corpus\n",
      "        if clean_corpus.ndim > 1:\n",
      "            clean_corpus = map(lambda phrase: clean(phrase), corpus[:,1]) # second column of corpus (Phrase)\n",
      "        corpus_sparse = super(SentAnaVectorizer, self).transform(clean_corpus)\n",
      "        \n",
      "        # customized transformation\n",
      "        vectors_list = [ self._phrase2vector(phraseId, phrase) for phraseId, phrase in corpus ]\n",
      "        vectors_sparse = sparse.csr_matrix(np.array(vectors_list))\n",
      "        \n",
      "        # append the two sparse matrices\n",
      "        corpus_sparse = hstack((corpus_sparse, vectors_sparse))\n",
      "        \n",
      "        # concatenate both results\n",
      "        return corpus_sparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Grid search hyper-parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a smaller set of the data to search for parameters  \n",
      "*(kernel crashes otherwise)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_gs = train[train['SentenceId'] < 50] # subset for testing purposes, limit to 1500 sentences\n",
      "X_train_gs = (train_gs[['PhraseId','Phrase']]).as_matrix()\n",
      "y_train_gs = train_gs['Sentiment']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up a pipeline and the hyper-parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = Pipeline([\n",
      "    ('vect', SentAnaVectorizer()),\n",
      "    ('clf', LogisticRegression())\n",
      "])\n",
      "\n",
      "parameters = {\n",
      "    'vect__binary': (True, False),\n",
      "    'vect__max_df': (0.5, 0.75, 1.0),\n",
      "    'vect__min_df': (1, 0.01),\n",
      "    'vect__use_afinns': (False,True),\n",
      "    'vect__use_embeddings': (False,True),\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect = SentAnaVectorizer(binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (X_train_gs.shape[1])\n",
      "arr = X_train_gs\n",
      "print len(arr.shape)\n",
      "print type(X_train_gs[:,1])\n",
      "print X_train_gs[:,1]\n",
      "arr.ndim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "2\n",
        "<type 'numpy.ndarray'>\n",
        "[ 'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
        " 'A series of escapades demonstrating the adage that what is good for the goose'\n",
        " 'A series' ..., 'memorable zingers' 'memorable' 'zingers']\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect.fit(X_train_gs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "SentAnaVectorizer(aggregate=<function sum at 0x104d40500>, analyzer=u'word',\n",
        "         binary=True, lowercase=True, max_df=0.5, max_features=None,\n",
        "         min_df=1, ngram_range=(1, 1), stop_words=u'english',\n",
        "         strip_accents=None, tokenizer=None, use_afinns=True,\n",
        "         use_embeddings=False, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "{u'100': 0,\n",
        " u'25': 1,\n",
        " u'absolute': 2,\n",
        " u'action': 3,\n",
        " u'actor': 4,\n",
        " u'actors': 5,\n",
        " u'actually': 6,\n",
        " u'adage': 7,\n",
        " u'age': 8,\n",
        " u'aggressive': 9,\n",
        " u'aims': 10,\n",
        " u'airless': 11,\n",
        " u'amounts': 12,\n",
        " u'amuses': 13,\n",
        " u'amusing': 14,\n",
        " u'apart': 15,\n",
        " u'appropriate': 16,\n",
        " u'arrives': 17,\n",
        " u'arthur': 18,\n",
        " u'arts': 19,\n",
        " u'avengers': 20,\n",
        " u'avoid': 21,\n",
        " u'bad': 22,\n",
        " u'banality': 23,\n",
        " u'bartlett': 24,\n",
        " u'baseball': 25,\n",
        " u'best': 26,\n",
        " u'betrayal': 27,\n",
        " u'better': 28,\n",
        " u'big': 29,\n",
        " u'bilingual': 30,\n",
        " u'boilerplate': 31,\n",
        " u'boy': 32,\n",
        " u'cartoon': 33,\n",
        " u'case': 34,\n",
        " u'cattaneo': 35,\n",
        " u'chance': 36,\n",
        " u'character': 37,\n",
        " u'characters': 38,\n",
        " u'charm': 39,\n",
        " u'charmer': 40,\n",
        " u'childhood': 41,\n",
        " u'chuck': 42,\n",
        " u'chuckles': 43,\n",
        " u'cinematic': 44,\n",
        " u'clearly': 45,\n",
        " u'cliched': 46,\n",
        " u'cliches': 47,\n",
        " u'combination': 48,\n",
        " u'comedy': 49,\n",
        " u'companion': 50,\n",
        " u'complications': 51,\n",
        " u'computer': 52,\n",
        " u'considerable': 53,\n",
        " u'considers': 54,\n",
        " u'constructed': 55,\n",
        " u'converted': 56,\n",
        " u'couples': 57,\n",
        " u'crime': 58,\n",
        " u'crisis': 59,\n",
        " u'damned': 60,\n",
        " u'dark': 61,\n",
        " u'dash': 62,\n",
        " u'date': 63,\n",
        " u'david': 64,\n",
        " u'day': 65,\n",
        " u'deceit': 66,\n",
        " u'decent': 67,\n",
        " u'defend': 68,\n",
        " u'demonstrating': 69,\n",
        " u'despite': 70,\n",
        " u'dialogue': 71,\n",
        " u'different': 72,\n",
        " u'directions': 73,\n",
        " u'disagree': 74,\n",
        " u'distort': 75,\n",
        " u'disturb': 76,\n",
        " u'dizzily': 77,\n",
        " u'does': 78,\n",
        " u'dogs': 79,\n",
        " u'drama': 80,\n",
        " u'drawn': 81,\n",
        " u'dream': 82,\n",
        " u'dreams': 83,\n",
        " u'early': 84,\n",
        " u'earnest': 85,\n",
        " u'easily': 86,\n",
        " u'ends': 87,\n",
        " u'engaging': 88,\n",
        " u'entertaining': 89,\n",
        " u'epic': 90,\n",
        " u'error': 91,\n",
        " u'escapades': 92,\n",
        " u'escapism': 93,\n",
        " u'ethnography': 94,\n",
        " u'ex': 95,\n",
        " u'exclusively': 96,\n",
        " u'exercise': 97,\n",
        " u'exists': 98,\n",
        " u'expect': 99,\n",
        " u'experimental': 100,\n",
        " u'extravagant': 101,\n",
        " u'familiar': 102,\n",
        " u'fans': 103,\n",
        " u'far': 104,\n",
        " u'fatal': 105,\n",
        " u'feature': 106,\n",
        " u'feel': 107,\n",
        " u'feels': 108,\n",
        " u'film': 109,\n",
        " u'filmmaker': 110,\n",
        " u'finish': 111,\n",
        " u'flick': 112,\n",
        " u'followed': 113,\n",
        " u'forces': 114,\n",
        " u'fortunately': 115,\n",
        " u'freakish': 116,\n",
        " u'french': 117,\n",
        " u'fresnadillo': 118,\n",
        " u'frighten': 119,\n",
        " u'frothing': 120,\n",
        " u'gag': 121,\n",
        " u'gallo': 122,\n",
        " u'gambles': 123,\n",
        " u'games': 124,\n",
        " u'gander': 125,\n",
        " u'generated': 126,\n",
        " u'girlfriend': 127,\n",
        " u'gives': 128,\n",
        " u'glacial': 129,\n",
        " u'glorification': 130,\n",
        " u'goal': 131,\n",
        " u'going': 132,\n",
        " u'good': 133,\n",
        " u'goose': 134,\n",
        " u'gorgeous': 135,\n",
        " u'grenade': 136,\n",
        " u'gritty': 137,\n",
        " u'hampered': 138,\n",
        " u'hard': 139,\n",
        " u'hate': 140,\n",
        " u'hatfield': 141,\n",
        " u'hell': 142,\n",
        " u'heroes': 143,\n",
        " u'hicks': 144,\n",
        " u'high': 145,\n",
        " u'hilarity': 146,\n",
        " u'home': 147,\n",
        " u'honestly': 148,\n",
        " u'hong': 149,\n",
        " u'horrifying': 150,\n",
        " u'horror': 151,\n",
        " u'host': 152,\n",
        " u'house': 153,\n",
        " u'ice': 154,\n",
        " u'impeccable': 155,\n",
        " u'importance': 156,\n",
        " u'incoherence': 157,\n",
        " u'indecipherable': 158,\n",
        " u'independent': 159,\n",
        " u'indication': 160,\n",
        " u'indie': 161,\n",
        " u'inducing': 162,\n",
        " u'indulgent': 163,\n",
        " u'inept': 164,\n",
        " u'inoffensive': 165,\n",
        " u'inspired': 166,\n",
        " u'interested': 167,\n",
        " u'intrigue': 168,\n",
        " u'introspective': 169,\n",
        " u'ismail': 170,\n",
        " u'jaunt': 171,\n",
        " u'jones': 172,\n",
        " u'joy': 173,\n",
        " u'judge': 174,\n",
        " u'judgment': 175,\n",
        " u'juicy': 176,\n",
        " u'just': 177,\n",
        " u'keeps': 178,\n",
        " u'kong': 179,\n",
        " u'kung': 180,\n",
        " u'latest': 181,\n",
        " u'leave': 182,\n",
        " u'like': 183,\n",
        " u'little': 184,\n",
        " u'love': 185,\n",
        " u'lrb': 186,\n",
        " u'mainland': 187,\n",
        " u'make': 188,\n",
        " u'makers': 189,\n",
        " u'makes': 190,\n",
        " u'mamet': 191,\n",
        " u'manipulative': 192,\n",
        " u'martial': 193,\n",
        " u'material': 194,\n",
        " u'meaning': 195,\n",
        " u'means': 196,\n",
        " u'meaty': 197,\n",
        " u'memorable': 198,\n",
        " u'merchant': 199,\n",
        " u'mess': 200,\n",
        " u'midlife': 201,\n",
        " u'mile': 202,\n",
        " u'minded': 203,\n",
        " u'minute': 204,\n",
        " u'minutes': 205,\n",
        " u'modern': 206,\n",
        " u'modest': 207,\n",
        " u'mongrel': 208,\n",
        " u'monty': 209,\n",
        " u'mood': 210,\n",
        " u'moonlight': 211,\n",
        " u'movie': 212,\n",
        " u'movies': 213,\n",
        " u'mr': 214,\n",
        " u'murder': 215,\n",
        " u'mythic': 216,\n",
        " u'narrative': 217,\n",
        " u'narratively': 218,\n",
        " u'nearly': 219,\n",
        " u'nerve': 220,\n",
        " u'new': 221,\n",
        " u'norris': 222,\n",
        " u'occasional': 223,\n",
        " u'occasionally': 224,\n",
        " u'occurs': 225,\n",
        " u'oddest': 226,\n",
        " u'oedekerk': 227,\n",
        " u'offering': 228,\n",
        " u'offers': 229,\n",
        " u'opera': 230,\n",
        " u'opportunities': 231,\n",
        " u'option': 232,\n",
        " u'pacing': 233,\n",
        " u'pages': 234,\n",
        " u'paralyzed': 235,\n",
        " u'party': 236,\n",
        " u'path': 237,\n",
        " u'pedigree': 238,\n",
        " u'pep': 239,\n",
        " u'peppering': 240,\n",
        " u'performance': 241,\n",
        " u'performances': 242,\n",
        " u'perspective': 243,\n",
        " u'perverse': 244,\n",
        " u'phrase': 245,\n",
        " u'playing': 246,\n",
        " u'plays': 247,\n",
        " u'plodding': 248,\n",
        " u'plot': 249,\n",
        " u'poetry': 250,\n",
        " u'point': 251,\n",
        " u'political': 252,\n",
        " u'positively': 253,\n",
        " u'pow': 254,\n",
        " u'powers': 255,\n",
        " u'preach': 256,\n",
        " u'primary': 257,\n",
        " u'progression': 258,\n",
        " u'proportions': 259,\n",
        " u'proves': 260,\n",
        " u'publishing': 261,\n",
        " u'purists': 262,\n",
        " u'quiet': 263,\n",
        " u'quotations': 264,\n",
        " u'rambling': 265,\n",
        " u'ramifications': 266,\n",
        " u'rattling': 267,\n",
        " u'reading': 268,\n",
        " u'realization': 269,\n",
        " u'reason': 270,\n",
        " u'recommend': 271,\n",
        " u'reigen': 272,\n",
        " u'relief': 273,\n",
        " u'remain': 274,\n",
        " u'remakes': 275,\n",
        " u'ride': 276,\n",
        " u'right': 277,\n",
        " u'role': 278,\n",
        " u'romantic': 279,\n",
        " u'romantics': 280,\n",
        " u'rooted': 281,\n",
        " u'rrb': 282,\n",
        " u'run': 283,\n",
        " u'runaway': 284,\n",
        " u'rut': 285,\n",
        " u'satire': 286,\n",
        " u'satisfying': 287,\n",
        " u'say': 288,\n",
        " u'schnitzler': 289,\n",
        " u'screen': 290,\n",
        " u'script': 291,\n",
        " u'seeking': 292,\n",
        " u'self': 293,\n",
        " u'sensational': 294,\n",
        " u'sense': 295,\n",
        " u'series': 296,\n",
        " u'serve': 297,\n",
        " u'setting': 298,\n",
        " u'shakespearean': 299,\n",
        " u'shell': 300,\n",
        " u'shiver': 301,\n",
        " u'shocker': 302,\n",
        " u'sitting': 303,\n",
        " u'slip': 304,\n",
        " u'smiles': 305,\n",
        " u'snow': 306,\n",
        " u'soap': 307,\n",
        " u'soon': 308,\n",
        " u'sounding': 309,\n",
        " u'source': 310,\n",
        " u'spectacularly': 311,\n",
        " u'start': 312,\n",
        " u'story': 313,\n",
        " u'storytelling': 314,\n",
        " u'structure': 315,\n",
        " u'study': 316,\n",
        " u'subject': 317,\n",
        " u'substitutable': 318,\n",
        " u'success': 319,\n",
        " u'sure': 320,\n",
        " u'suspect': 321,\n",
        " u'sweet': 322,\n",
        " u'tackled': 323,\n",
        " u'takes': 324,\n",
        " u'tartakovsky': 325,\n",
        " u'team': 326,\n",
        " u'terms': 327,\n",
        " u'terror': 328,\n",
        " u'theater': 329,\n",
        " u'thrilling': 330,\n",
        " u'throw': 331,\n",
        " u'time': 332,\n",
        " u'times': 333,\n",
        " u'title': 334,\n",
        " u'totally': 335,\n",
        " u'tragedy': 336,\n",
        " u'tricky': 337,\n",
        " u'trouble': 338,\n",
        " u'true': 339,\n",
        " u'try': 340,\n",
        " u'ultimately': 341,\n",
        " u'undergoing': 342,\n",
        " u'unexpected': 343,\n",
        " u'unless': 344,\n",
        " u'unnamed': 345,\n",
        " u'usual': 346,\n",
        " u'vapid': 347,\n",
        " u'vaudeville': 348,\n",
        " u'vincent': 349,\n",
        " u'visual': 350,\n",
        " u'watching': 351,\n",
        " u'ways': 352,\n",
        " u'weirdo': 353,\n",
        " u'welcome': 354,\n",
        " u'west': 355,\n",
        " u'whitewash': 356,\n",
        " u'wild': 357,\n",
        " u'wilde': 358,\n",
        " u'windtalkers': 359,\n",
        " u'winning': 360,\n",
        " u'wit': 361,\n",
        " u'woman': 362,\n",
        " u'wong': 363,\n",
        " u'work': 364,\n",
        " u'works': 365,\n",
        " u'world': 366,\n",
        " u'worth': 367,\n",
        " u'writers': 368,\n",
        " u'year': 369,\n",
        " u'young': 370,\n",
        " u'youth': 371,\n",
        " u'zingers': 372}"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start grid search and display results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=3, scoring='f1')\n",
      "grid_search.fit(X_train_gs, y_train_gs)\n",
      "\n",
      "print 'Best score:', grid_search.best_score_\n",
      "print 'Best parameters set:'\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print '\\t', param_name, best_parameters[param_name]\n",
      "    \n",
      "endtime = datetime.datetime.now()\n",
      "print '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
        "[CV] vect__use_afinns=False, vect__binary=True, vect__min_df=1, vect__use_embeddings=False, vect__max_df=0.5 \n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'lower'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-5dba0d2cfdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_gs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_gs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Best score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    376\u001b[0m                                     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                                     self.fit_params, return_parameters=True)\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             for train, test in cv)\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdata\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 234\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Set up and fit transformer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fit and transform the train set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = SentAnaVectorizer(binary=True, \n",
      "                               lowercase=True, \n",
      "                               ngram_range=(1, 2), \n",
      "                               use_afinns=True, \n",
      "                               use_embeddings=True, \n",
      "                               max_df=0.5,\n",
      "                               min_df=1,\n",
      "                               tokenizer=None) # StemTokenizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myx = train[['PhraseId','Phrase']]\n",
      "myy = train['Sentiment']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myx1, myx2, myy1, myy2 = train_test_split(myx, myy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(myx1)\n",
      "print myx1.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n",
        "(117045, 2)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print myx1[:,1]\n",
      "print type(myx1[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['to remake Sleepless in Seattle again and again' \"Mira Nair 's new movie\"\n",
        " 'of the dilemma' ..., 'An hour and a half of joyful solo performance .'\n",
        " 'the question how much souvlaki can you take before indigestion sets in'\n",
        " ', crazy']\n",
        "<type 'numpy.ndarray'>\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l = np.array(map(lambda phrase: clean(phrase), myx1[:,1]))\n",
      "print type(l)\n",
      "print l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n",
        "['to remake Sleepless in Seattle again and again' 'Mira Nair s new movie'\n",
        " 'of the dilemma' ..., 'An hour and a half of joyful solo performance'\n",
        " 'the question how much souvlaki can you take before indigestion sets in'\n",
        " 'crazy']\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "X_train_v = vectorizer.fit_transform(X_train)\n",
      "\n",
      "endtime = datetime.datetime.now()\n",
      "print 'Run time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'lower'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-28-c606b706eed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 234\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in vectorizer.vocabulary_.keys()[0:10]:\n",
      "    print k, vectorizer.vocabulary_[k]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'SentAnaVectorizer' object has no attribute 'vocabulary_'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-29-1abd30f9c62a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'SentAnaVectorizer' object has no attribute 'vocabulary_'"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Type of X_train_v: ', type(X_train_v)\n",
      "print 'Shape of X_train_v: ', X_train_v.shape\n",
      "#print X_train_v[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Type of X_train_v:  <class 'scipy.sparse.coo.coo_matrix'>\n",
        "Shape of X_train_v:  (117045, 73197)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Transform the test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "X_test_v = vectorizer.transform(X_test)\n",
      "print 'Type of X_test_v: ', type(X_test_v)\n",
      "print 'Shape of X_test_v: ', X_test_v.shape\n",
      "#print X_test_v[0:5]\n",
      "\n",
      "endtime = datetime.datetime.now()\n",
      "print '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Type of X_test_v:  <class 'scipy.sparse.coo.coo_matrix'>\n",
        "Shape of X_test_v:  (39015, 73197)\n",
        "\n",
        "Run time (secs):  28.0\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Set up and run model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "classifier = LogisticRegression()\n",
      "classifier.fit(X_train_v, y_train)\n",
      "\n",
      "endtime = datetime.datetime.now()\n",
      "print 'Run time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Run time (secs):  135.0\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluate the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict the y for the test data\n",
      "predictions = classifier.predict(X_test_v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10):\n",
      "    print 'actual: ', y_test[i], '\\tpredict: ', predictions[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actual:  2 \tpredict:  2\n",
        "actual:  0 \tpredict:  1\n",
        "actual:  2 \tpredict:  2\n",
        "actual:  2 \tpredict:  2\n",
        "actual:  2 \tpredict:  2\n",
        "actual:  3 \tpredict:  3\n",
        "actual:  4 \tpredict:  4\n",
        "actual:  2 \tpredict:  2\n",
        "actual:  2 \tpredict:  1\n",
        "actual:  3 \tpredict:  2\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Score the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classification_report(y_test, predictions)\n",
      "print 'accuracy:\\t', classifier.score(X_test_v, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.60      0.17      0.26      1763\n",
        "          1       0.54      0.36      0.43      6762\n",
        "          2       0.66      0.89      0.76     19876\n",
        "          3       0.59      0.45      0.51      8319\n",
        "          4       0.63      0.23      0.34      2295\n",
        "\n",
        "avg / total       0.62      0.63      0.60     39015\n",
        "\n",
        "accuracy:\t0.63434576445"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Random Forest Classifier\n",
      "***(Goes out of memory ...)***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Grid search hyper-parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first convert sparse matrix to dense matrix\n",
      "X_train_gs_v = (vectorizer.transform(X_train_gs)).todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up a pipeline and the hyper-parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline2 = Pipeline([\n",
      "    ('clf', RandomForestClassifier(bootstrap = False,criterion='entropy',n_jobs=-1))\n",
      "])\n",
      "\n",
      "n_features = int(math.ceil(math.sqrt(X_train_gs.shape[1])))\n",
      "parameters2 = {\n",
      "    'clf__n_estimators': (50, n_features),\n",
      "    'clf__max_depth': (50,100),\n",
      "    'clf__min_samples_split': (1, 2),\n",
      "    'clf__min_samples_leaf': (1, 10)\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start grid search and display results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "grid_search2 = GridSearchCV(pipeline2, parameters2, n_jobs=1, verbose=3, scoring='f1')\n",
      "grid_search2.fit(X_train_gs_v, y_train_gs)\n",
      "\n",
      "print 'Best score:', grid_search2.best_score_\n",
      "print 'Best parameters set:'\n",
      "best_parameters2 = grid_search2.best_estimator_.get_params()\n",
      "for param_name in sorted(parameters2.keys()):\n",
      "    print '\\t', param_name, best_parameters2[param_name]\n",
      "\n",
      "endtime = datetime.datetime.now()\n",
      "print '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Set up and run model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starttime = datetime.datetime.now()\n",
      "\n",
      "classifier2 = RandomForestClassifier(\n",
      "    n_estimators = n_features,\n",
      "    max_depth = None, # 50\n",
      "    min_samples_leaf = 1, #10\n",
      "    criterion='entropy',\n",
      "    n_jobs = -1,\n",
      "    bootstrap = False,\n",
      "    verbose = 1)\n",
      "\n",
      "classifier2.fit(X_train_v, y_train)\n",
      "\n",
      "endtime = datetime.datetime.now()\n",
      "print 'RandomForest: Duration (secs): ', timestamp(endtime) - timestamp(starttime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluate the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions2 = classifier2.predict(X_test_v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10):\n",
      "    print 'actual: ', y_test[i], '\\tpredict: ', predictionss[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Score the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classification_report(y_test, predictions2)\n",
      "print 'accuracy:\\t', classifier2.score(X_test_v, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Best results were obtained using a Logistic Regression model.  \n",
      "Issues encountered were mostly related to performance of running the Random Forest model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
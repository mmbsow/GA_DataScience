{
 "metadata": {
  "name": "",
  "signature": "sha256:f7ddc633ababa449ad89533514a879c7dc9f52b8b98119d50c204db7174dd2ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Document Classification Lab"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basic Text Feature Extraction with the Bag-of-Words Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create feature representations for the following corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "    'UNC played Duke in basketball',\n",
      "    'Duke lost the basketball game'\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corpus is represented as a `List` of documents. The documents are represented as `String`s. The bag-of-words representation requires operating on individual words. We must *tokenize* the documents. A token is a meaningful, atomic unit of text. Tokens are often words, but can also be affixes, punctuation, etc.  \n",
      "Our documents only contains words, so we can tokenize the documents by splitting the `String`s on whitespace. The `split(delimiter)` method of `String`s can be used to tokenize documents on spaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a `Set` of the words in the corpus.\n",
      "vocabulary_1 = set([w for l in [s.split() for s in corpus] for w in l])\n",
      "print vocabulary_1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['Duke', 'basketball', 'lost', 'played', 'game', 'UNC', 'in', 'the'])\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's map the elements of the `Set` to indices, and create feature representations for our corpus by iterating through the tokens in each document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zip(range(len(vocabulary_1)), vocabulary_1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[(0, 'Duke'),\n",
        " (1, 'basketball'),\n",
        " (2, 'lost'),\n",
        " (3, 'played'),\n",
        " (4, 'game'),\n",
        " (5, 'UNC'),\n",
        " (6, 'in'),\n",
        " (7, 'the')]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a dictionary that maps each token in the vocabulary to an index\n",
      "dictionary_1 = dict(zip(vocabulary_1, range(len(vocabulary_1))))\n",
      "print dictionary_1\n",
      "# TODO Now create a feature representation for each document in the corpus.\n",
      "# The feature representations can be lists of integers.\n",
      "X = []\n",
      "for sentence in corpus:\n",
      "    features = [0 for i in dictionary_1]\n",
      "    for t in sentence.split():\n",
      "        features[dictionary_1[t]] = 1 if t in dictionary_1 else 0\n",
      "    X += [features]\n",
      "# TODO print the feature representations\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'Duke': 0, 'basketball': 1, 'lost': 2, 'played': 3, 'game': 4, 'UNC': 5, 'in': 6, 'the': 7}\n",
        "[[1, 1, 0, 1, 0, 1, 1, 0], [1, 1, 1, 0, 1, 0, 0, 1]]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the CountVectorizer transformer from sklearn.feature_extraction.text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "# Create an instance of CountVectorizer. Set the keyword argument `binary` to True.\n",
      "binary_vectorizer = CountVectorizer(binary=True)\n",
      "# Fit the vectorizer on the corpus, and transform the corpus\n",
      "X = binary_vectorizer.fit_transform(corpus)\n",
      "# TODO Print the vectorizer's vocabulary. Is the collection the same as the vocabulary you created?\n",
      "print binary_vectorizer.vocabulary_\n",
      "# TODO Print the transformed feature representations. \n",
      "X.todense()\n",
      "# You can print the dense matrix using the method `todense()`.\n",
      "# The vectors may not be the same as the elements are not ordered as the words were encountered."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "matrix([[1, 1, 0, 1, 0, 1, 0, 1],\n",
        "        [1, 1, 1, 0, 1, 0, 1, 0]])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectors that have many zero-valued elements are called *sparse vectors*. Sparse matrices can be represented efficiently by storing only the indices and values of non-zero elements.  \n",
      "`CountVectorizer` returns sparse matrices by default. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now add a third document, \"I ate a sandwich,\" to the corpus.\n",
      "corpus += [\"I ate a sandwich,\"]\n",
      "print corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich,']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO fit the vectorizer again and print the vocabulary. What words should you expect to be added?\n",
      "binary_vectorizer.fit_transform(corpus)\n",
      "print binary_vectorizer.vocabulary_\n",
      "# TODO are any words missing? (Spoiler: yes). \n",
      "# Consult the documentation for the class to learn why: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
      "# TODO transform the corpus and print the feature representations again.\n",
      "X = binary_vectorizer.transform(corpus)\n",
      "X.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "matrix([[0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
        "        [0, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
        "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is the first document more similar to the second or third documents? Do the feature representations encode this similarity?  \n",
      "\n",
      "Recall that the L^2 norm of a vector is given by the following equation: \n",
      "\n",
      "$$||x||_2 = \\sqrt{x_1^2 + x_2^2 , \\dotsc, x_n^2}$$\n",
      "\n",
      "Let's compare the documents in the corpus using their distances from each other in space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "def print_distances(X):\n",
      "    for i in range(0, len(X.todense())):\n",
      "        for j in range(i+1, len(X.todense())):\n",
      "            print 'The Euclidean distance between [%s] and [%s] is [%s]' % (corpus[i], corpus[j], euclidean_distances(X[i], X[j]))\n",
      "            \n",
      "print_distances(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Euclidean distance between [UNC played Duke in basketball] and [Duke lost the basketball game] is [[[ 2.44948974]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [I ate a sandwich,] is [[[ 2.64575131]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [I ate a sandwich,] is [[[ 2.64575131]]]\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first two documents pertain to college basketball. The third document does not. Accordingly, the first two documents are closer to each other than they are to the third document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO add two documents to the corpus. \n",
      "corpus += [\"I played basketball at Duke\", \"The sandwich I ate at the game was good\"]\n",
      "# One document should be more similar to the first document than the others, \n",
      "# and the other document should be more similar to the third document than the others.\n",
      "# TODO Measure their distances.\n",
      "binary_vectorizer.fit(corpus)\n",
      "print binary_vectorizer.vocabulary_\n",
      "X = binary_vectorizer.transform(corpus)\n",
      "X.todense()\n",
      "\n",
      "print_distances(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'duke': 3, u'basketball': 2, u'lost': 7, u'played': 8, u'in': 6, u'game': 4, u'good': 5, u'sandwich': 9, u'unc': 11, u'ate': 1, u'the': 10, u'was': 12, u'at': 0}\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [Duke lost the basketball game] is [[[ 2.44948974]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [I ate a sandwich,] is [[[ 2.64575131]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [I played basketball at Duke] is [[[ 1.73205081]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [The sandwich I ate at the game was good] is [[[ 3.46410162]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [I ate a sandwich,] is [[[ 2.64575131]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [I played basketball at Duke] is [[[ 2.23606798]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [The sandwich I ate at the game was good] is [[[ 2.82842712]]]\n",
        "The Euclidean distance between [I ate a sandwich,] and [I played basketball at Duke] is [[[ 2.44948974]]]\n",
        "The Euclidean distance between [I ate a sandwich,] and [The sandwich I ate at the game was good] is [[[ 2.23606798]]]\n",
        "The Euclidean distance between [I played basketball at Duke] and [The sandwich I ate at the game was good] is [[[ 3.]]]\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Document Classification\n",
      "\n",
      "Let's use our bag-of-words feature representations with a logistic regressor to classify documents. We will build a contemporary version of the canonical supervised machine learning application, the spam classifier. Instead of classifying email, we will classify SMS messages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loading and Inspecting the Data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Import numpy as np, pandas as pd\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# TODO read `sms.csv` into a `DataFrame`.\n",
      "df = pd.read_csv(\"sms.csv\")\n",
      "\n",
      "# TODO what are the dataframe's columns?\n",
      "print df.columns\n",
      "\n",
      "# TODO are the classes balanced? Use the `value_counts()` method to count the number of spam and ham messages.\n",
      "print df['label'].value_counts()\n",
      "\n",
      "# TODO inspect the data. Print the first 10 spam and ham messages\n",
      "print df[df['label'] == 1].head(10)\n",
      "print df[df['label'] == 0].head(10)\n",
      "\n",
      "# TODO assign the 'label' column to y, and the 'message' column to X.\n",
      "y_raw = df['label']\n",
      "X_raw = df['message']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'label', u'message'], dtype='object')\n",
        "0    4827\n",
        "1     747\n",
        "dtype: int64\n",
        "    label                                            message\n",
        "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "5       1  FreeMsg Hey there darling it's been 3 week's n...\n",
        "8       1  WINNER!! As a valued network customer you have...\n",
        "9       1  Had your mobile 11 months or more? U R entitle...\n",
        "11      1  SIX chances to win CASH! From 100 to 20,000 po...\n",
        "12      1  URGENT! You have won a 1 week FREE membership ...\n",
        "15      1  XXXMobileMovieClub: To use your credit, click ...\n",
        "19      1  England v Macedonia - dont miss the goals/team...\n",
        "34      1  Thanks for your subscription to Ringtone UK yo...\n",
        "42      1  07732584351 - Rodger Burns - MSG = We tried to...\n",
        "    label                                            message\n",
        "0       0  Go until jurong point, crazy.. Available only ...\n",
        "1       0                      Ok lar... Joking wif u oni...\n",
        "3       0  U dun say so early hor... U c already then say...\n",
        "4       0  Nah I don't think he goes to usf, he lives aro...\n",
        "6       0  Even my brother is not like to speak with me. ...\n",
        "7       0  As per your request 'Melle Melle (Oru Minnamin...\n",
        "10      0  I'm gonna be home soon and i don't want to tal...\n",
        "13      0  I've been searching for the right words to tha...\n",
        "14      0                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
        "16      0                         Oh k...i'm watching here:)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partitioning and Transforming the Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `train_test_split` convenience function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "# TODO split the data set into training and testing sets\n",
      "# X_train, X_test, y_train, y_test = \n",
      "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, random_state=123)\n",
      "\n",
      "# TODO fit the `binary_vectorizer` on the training set\n",
      "binary_vectorizer.fit(X_train)\n",
      "\n",
      "# TODO how many words are in the vocabulary?\n",
      "print len(binary_vectorizer.vocabulary_)\n",
      "\n",
      "# TODO transform the training set\n",
      "X_train = binary_vectorizer.transform(X_train)\n",
      "\n",
      "# TODO transform the test set\n",
      "X_test = binary_vectorizer.transform(X_test)\n",
      "\n",
      "# TODO print X_train\n",
      "print X_train[:5].todense()\n",
      "\n",
      "# TODO what are the dimensions of the design matrix?\n",
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7447\n",
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "(4180, 7447)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the Model "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `LogisticRegression` class from the `linear_model` module.\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# TODO create an instance of `LogisticRegression`\n",
      "classifier = LogisticRegression()\n",
      "\n",
      "# TODO fit the classifier on the training data\n",
      "classifier.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating the Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO evaluate the classifier. What is the default performance measure for `LogisticRegression`? \n",
      "# (Hint: consult the documentation)\n",
      "print classifier.decision_function(X_test)\n",
      "# Let's evaluate the classifier using other performance measures.\n",
      "# TODO import the `recall_score`, `precision_score`, and `f1_score` functions from the `metrics` module.\n",
      "from sklearn.metrics import recall_score, precision_score, f1_score\n",
      "# TODO make predictions for the test set\n",
      "predictions = classifier.predict(X_test)\n",
      "probabilities = classifier.predict_proba(X_test)\n",
      "for p in predictions[:5]:\n",
      "    print p\n",
      "for p in probabilities[:5]:\n",
      "    print p\n",
      "# TODO evaluate the classifier's precision, recall, and F1 score.\n",
      "print 'Accuracy = ', classifier.score(X_test, y_test)\n",
      "print 'Precision = ', precision_score(y_test, predictions)\n",
      "print 'Recall = ', recall_score(y_test, predictions)\n",
      "print 'F1 = ', f1_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [-4.62409891 -3.73604382 -5.82272259 ..., -4.98198109 -4.88153461\n",
        " -5.8073707 ]\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "[ 0.99028286  0.00971714]\n",
        "[ 0.97670723  0.02329277]\n",
        "[ 0.9970492  0.0029508]\n",
        "[ 0.99631344  0.00368656]\n",
        "[ 0.00510887  0.99489113]\n",
        "Accuracy =  0.984218077475\n",
        "Precision =  0.994565217391\n",
        "Recall =  0.897058823529\n",
        "F1 =  0.943298969072\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluating the classifier on a single training and test partition doesn't provide the best estimate of its performance. What if the class proportions are imbalanced, and all of the instances of one class appear only in one of the partitions?  \n",
      "Instead, we will evaluate the model using *cross validation*. We will make `n` partitions. We will train on `n-1` partitions and evaluate on the nth partition. We will then rotate the partitions, retrain, and re-test. \n",
      "We will continue rotating the partitions until we have trained and evaluated using all of the partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `cross_val_score` function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "# `cross_val_score()` takes the estimator object, the design matrix, and the values of the response variable.\n",
      "# It also takes the `cv` keyword argument to set the number of cross validation partitions.\n",
      "# `cross_val_score()` will train and test with each partition.\n",
      "# This means that we cannot fit the `CountVectorizer` once; we must fit it for each cross validation fold.\n",
      "# We can combine the `CountVectorizer` and the classifier to form a `Pipeline`. We can then pass the `Pipeline` to `cross_val_score()`.\n",
      "\n",
      "# TODO import the class `Pipeline` from the module `pipeline`.\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "# The constructor for `Pipeline` takes a list of tuples.\n",
      "# The first element of each tuple is a name for the step that you create.\n",
      "# The second element is the object that performs the step.\n",
      "# TODO create a `Pipeline` that converts Strings representing documents to binary count vectors\n",
      "# before classifying them using logistic regression.\n",
      "# TODO call the instance of `CountVectorizer` \"vect\" and the instace of `LogisticRegression` \"clf\".\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(binary=True)),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "\n",
      "# TODO evaluate the pipeline using 5-fold cross validation. Pass the pipeline, X_raw, and y_raw to the function.\n",
      "print 'Accuracy = ', cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "\n",
      "# TODO now evaluate the classifier's `recall`, `precision`, and `F1` score using cross validation.\n",
      "print 'F1 = ', cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='f1')\n",
      "print 'Recall = ', cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print 'Precision = ', cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy =  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.98476703  0.98566308  0.98114901  0.98294434  0.98294434]\n",
        "F1 =  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.93992933  0.94366197  0.92473118  0.93189964  0.93238434]\n",
        "Recall =  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.88666667  0.89333333  0.86577181  0.87248322  0.87919463]\n",
        "Precision =  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.          1.          0.99230769  1.          0.99242424]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn\n",
      "\n",
      "Find or create a set of documents for a binary classification problem. Produce feature representations for your documents using the bag-of-words model, and classify the documents using logistic regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "text = None\n",
      "with open(\"mytext.txt\", 'r') as stream:\n",
      "    text = stream.read()\n",
      "    \n",
      "binary_vectorizer = CountVectorizer(binary=True)\n",
      "# Fit the vectorizer on the corpus, and transform the corpus\n",
      "X2 = binary_vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeDecodeError",
       "evalue": "'utf8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-81-adb33198d970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbinary_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fit the vectorizer on the corpus, and transform the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 233\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Anaconda/anaconda/python.app/Contents/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   label                                            message\n",
        "0      0  Go until jurong point, crazy.. Available only ...\n",
        "1      0                      Ok lar... Joking wif u oni...\n",
        "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "3      0  U dun say so early hor... U c already then say...\n",
        "4      0  Nah I don't think he goes to usf, he lives aro...\n",
        "5      1  FreeMsg Hey there darling it's been 3 week's n...\n",
        "6      0  Even my brother is not like to speak with me. ...\n",
        "7      0  As per your request 'Melle Melle (Oru Minnamin...\n",
        "8      1  WINNER!! As a valued network customer you have...\n",
        "9      1  Had your mobile 11 months or more? U R entitle...\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:64f60a98a116fad1a321a8fa9357c3abb01d10276fef4cb59ffbd39901f0f634"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Document Classification Lab"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basic Text Feature Extraction with the Bag-of-Words Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create feature representations for the following corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "    'UNC played Duke in basketball',\n",
      "    'Duke lost the basketball game'\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corpus is represented as a `List` of documents. The documents are represented as `String`s. The bag-of-words representation requires operating on individual words. We must *tokenize* the documents. A token is a meaningful, atomic unit of text. Tokens are often words, but can also be affixes, punctuation, etc.  \n",
      "Our documents only contains words, so we can tokenize the documents by splitting the `String`s on whitespace. The `split(delimiter)` method of `String`s can be used to tokenize documents on spaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a `Set` of the words in the corpus.\n",
      "vocabulary_1 = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's map the elements of the `Set` to indices, and create feature representations for our corpus by iterating through the tokens in each document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a dictionary that maps each token in the vocabulary to an index\n",
      "dictionary_1 = \n",
      "# TODO Now create a feature representation for each document in the corpus.\n",
      "# The feature representations can be lists of integers.\n",
      "X = []\n",
      "\n",
      "# TODO print the feature representations\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the CountVectorizer transformer from sklearn.feature_extraction.text\n",
      "\n",
      "# Create an instance of CountVectorizer. Set the keyword argument `binary` to True.\n",
      "binary_vectorizer = \n",
      "# Fit the vectorizer on the corpus, and transform the corpus\n",
      "X = \n",
      "# TODO Print the vectorizer's vocabulary. Is the collection the same as the vocabulary you created?\n",
      "# TODO Print the transformed feature representations. \n",
      "# You can print the dense matrix using the method `todense()`.\n",
      "# The vectors may not be the same as the elements are not ordered as the words were encountered."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectors that have many zero-valued elements are called *sparse vectors*. Sparse matrices can be represented efficiently by storing only the indices and values of non-zero elements.  \n",
      "`CountVectorizer` returns sparse matrices by default. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now add a third document, \"I ate a sandwich,\" to the corpus.\n",
      "corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO fit the vectorizer again and print the vocabulary. What words should you expect to be added?\n",
      "\n",
      "# TODO are any words missing? (Spoiler: yes). \n",
      "# Consult the documentation for the class to learn why: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
      "# TODO transform the corpus and print the feature representations again.\n",
      "X = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is the first document more similar to the second or third documents? Do the feature representations encode this similarity?  \n",
      "\n",
      "Recall that the L^2 norm of a vector is given by the following equation: \n",
      "\n",
      "$$||x||_2 = \\sqrt{x_1^2 + x_2^2 , \\dotsc, x_n^2}$$\n",
      "\n",
      "Let's compare the documents in the corpus using their distances from each other in space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "for i in range(0, len(X.todense())):\n",
      "    for j in range(i+1, len(X.todense())):\n",
      "        print 'The Euclidean distance between [%s] and [%s] is [%s]' % (corpus[i], corpus[j], euclidean_distances(X[i], X[j]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first two documents pertain to college basketball. The third document does not. Accordingly, the first two documents are closer to each other than they are to the third document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO add two documents to the corpus. \n",
      "# One document should be more similar to the first document than the others, \n",
      "# and the other document should be more similar to the third document than the others.\n",
      "# TODO Measure their distances."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Document Classification\n",
      "\n",
      "Let's use our bag-of-words feature representations with a logistic regressor to classify documents. We will build a contemporary version of the canonical supervised machine learning application, the spam classifier. Instead of classifying email, we will classify SMS messages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loading and Inspecting the Data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Import numpy as np, pandas as pd\n",
      "\n",
      "# TODO read `sms.csv` into a `DataFrame`.\n",
      "df = \n",
      "# TODO what are the dataframe's columns?\n",
      "\n",
      "# TODO are the classes balanced? Use the `value_counts()` method to count the number of spam and ham messages.\n",
      "\n",
      "# TODO inspect the data. Print the first 10 spam and ham messages\n",
      "\n",
      "# TODO assign the 'label' column to y, and the 'message' column to X.\n",
      "y_raw = \n",
      "X_raw = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partitioning and Transforming the Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `train_test_split` convenience function from the `cross_validation` module.\n",
      "\n",
      "# TODO split the data set into training and testing sets\n",
      "# X_train, X_test, y_train, y_test = \n",
      "X_train, X_test, y_train, y_test = \n",
      "# TODO fit the `binary_vectorizer` on the training set\n",
      "X_train = \n",
      "# TODO how many words are in the vocabulary?\n",
      "\n",
      "# TODO transform the training set\n",
      "X_test = \n",
      "# TODO transform the test set\n",
      "X_test = \n",
      "# TODO print X_train\n",
      "\n",
      "# TODO what are the dimensions of the design matrix?\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the Model "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `LogisticRegression` class from the `linear_model` module.\n",
      "\n",
      "# TODO create an instance of `LogisticRegression`\n",
      "classifier = \n",
      "# TODO fit the classifier on the training data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating the Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO evaluate the classifier. What is the default performance measure for `LogisticRegression`? \n",
      "# (Hint: consult the documentation)\n",
      "\n",
      "# Let's evaluate the classifier using other performance measures.\n",
      "# TODO import the `recall_score`, `precision_score`, and `f1_score` functions from the `metrics` module.\n",
      "\n",
      "# TODO make predictions for the test set\n",
      "predictions = \n",
      "# TODO evaluate the classifier's precision, recall, and F1 score.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluating the classifier on a single training and test partition doesn't provide the best estimate of its performance. What if the class proportions are imbalanced, and all of the instances of one class appear only in one of the partitions?  \n",
      "Instead, we will evaluate the model using *cross validation*. We will make `n` partitions. We will train on `n-1` partitions and evaluate on the nth partition. We will then rotate the partitions, retrain, and re-test. \n",
      "We will continue rotating the partitions until we have trained and evaluated using all of the partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `cross_val_score` function from the `cross_validation` module.\n",
      "\n",
      "# `cross_val_score()` takes the estimator object, the design matrix, and the values of the response variable.\n",
      "# It also takes the `cv` keyword argument to set the number of cross validation partitions.\n",
      "# `cross_val_score()` will train and test with each partition.\n",
      "# This means that we cannot fit the `CountVectorizer` once; we must fit it for each cross validation fold.\n",
      "# We can combine the `CountVectorizer` and the classifier to form a `Pipeline`. We can then pass the `Pipeline` to `cross_val_score()`.\n",
      "# TODO import the class `Pipeline` from the module `pipeline`.\n",
      "# The constructor for `Pipeline` takes a list of tuples.\n",
      "# The first element of each tuple is a name for the step that you create.\n",
      "# The second element is the object that performs the step.\n",
      "# TODO create a `Pipeline` that converts Strings representing documents to binary count vectors\n",
      "# before classifying them using logistic regression.\n",
      "# TODO call the instance of `CountVectorizer` \"vect\" and the instace of `LogisticRegression` \"clf\".\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(binary=True)),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "# TODO evaluate the pipeline using 5-fold cross validation. Pass the pipeline, X_raw, and y_raw to the function.\n",
      "\n",
      "# TODO now evaluate the classifier's `recall`, `precision`, and `F1` score using cross validation.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn\n",
      "\n",
      "Find or create a set of documents for a binary classification problem. Produce feature representations for your documents using the bag-of-words model, and classify the documents using logistic regression."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
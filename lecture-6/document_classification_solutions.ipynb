{
 "metadata": {
  "name": "",
  "signature": "sha256:9f3eb6c2ebe2e7ec6f66f995270c4a9c4f0e00c4a73601cb3ab339cb2cf4f7c3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Document Classification Lab"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basic Text Feature Extraction with the Bag-of-Words Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create feature representations for the following corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "    'UNC played Duke in basketball',\n",
      "    'Duke lost the basketball game'\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corpus is represented as a `List` of documents. The documents are represented as `String`s. The bag-of-words representation requires operating on individual words. We must *tokenize* the documents. A token is a meaningful, atomic unit of text. Tokens are often words, but can also be affixes, punctuation, etc.  \n",
      "Our documents only contains words, so we can tokenize the documents by splitting the `String`s on whitespace. The `split(delimiter)` method of `String`s can be used to tokenize documents on spaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a `Set` of the words in the corpus.\n",
      "vocabulary_1 = set(corpus[0].split(' ') + corpus[1].split(' '))\n",
      "print vocabulary_1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['Duke', 'basketball', 'lost', 'played', 'game', 'UNC', 'in', 'the'])\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's map the elements of the `Set` to indices, and create feature representations for our corpus by iterating through the tokens in each document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a dictionary that maps each token in the vocabulary to an index\n",
      "dictionary_1 = {token: i for (i, token) in enumerate(vocabulary_1)}\n",
      "# TODO Now create a feature representation for each document in the corpus.\n",
      "# The feature representations can be lists of integers.\n",
      "X = []\n",
      "for sentence in corpus:\n",
      "    x = [0] * len(vocabulary_1)\n",
      "    for token in sentence.split(' '):\n",
      "        x[dictionary_1[token]] = 1\n",
      "    X.append(x)\n",
      "# TODO print the feature representations\n",
      "print X\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1, 1, 0, 1, 0, 1, 1, 0], [1, 1, 1, 0, 1, 0, 0, 1]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the CountVectorizer transformer from sklearn.feature_extraction.text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "# Create an instance of CountVectorizer. Set the keyword argument `binary` to True.\n",
      "# binary_vectorizer = \n",
      "binary_vectorizer = CountVectorizer(binary=True)\n",
      "# Fit the vectorizer on the corpus, and transform the corpus\n",
      "# X = \n",
      "X = binary_vectorizer.fit_transform(corpus)\n",
      "# TODO Print the vectorizer's vocabulary. Is the collection the same as the vocabulary you created?\n",
      "# TODO Print the transformed feature representations. You can print the dense matrix using the method `todense()`.\n",
      "# The vectors may not be the same as the elements are not ordered as the words were encountered.\n",
      "print X.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1 0 1 0 1 0 1]\n",
        " [1 1 1 0 1 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectors that have many zero-valued elements are called *sparse vectors*. Sparse matrices can be represented efficiently by storing only the indices and values of non-zero elements.  \n",
      "`CountVectorizer` returns sparse matrices by default. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now add a third document, \"I ate a sandwich,\" to the corpus.\n",
      "# corpus\n",
      "corpus.append('I ate a sandwich')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO fit the vectorizer again and print the vocabulary. What words should you expect to be added?\n",
      "binary_vectorizer.fit(corpus)\n",
      "print binary_vectorizer.vocabulary_\n",
      "# TODO are any words missing? (Spoiler: yes). \n",
      "# Consult the documentation for the class to learn why: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
      "# TODO transform the corpus and print the feature representations again.\n",
      "X = binary_vectorizer.transform(corpus)\n",
      "print binary_vectorizer.transform(corpus).todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n",
        "[[0 1 1 0 1 0 1 0 0 1]\n",
        " [0 1 1 1 0 1 0 0 1 0]\n",
        " [1 0 0 0 0 0 0 1 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is the first document more similar to the second or third documents? Do the feature representations encode this similarity?  \n",
      "\n",
      "Recall that the L^2 norm of a vector is given by the following equation: \n",
      "\n",
      "$$||x||_2 = \\sqrt{x_1^2 + x_2^2 , \\dotsc, x_n^2}$$\n",
      "\n",
      "Let's compare the documents in the corpus using their distances from each other in space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "for i in range(0, len(X.todense())):\n",
      "    for j in range(i+1, len(X.todense())):\n",
      "        print 'The Euclidean distance between [%s] and [%s] is [%s]' % (corpus[i], corpus[j], euclidean_distances(X[i], X[j]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Euclidean distance between [UNC played Duke in basketball] and [Duke lost the basketball game] is [[[ 2.44948974]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [I ate a sandwich] is [[[ 2.64575131]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [I ate a sandwich] is [[[ 2.64575131]]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first two documents pertain to college basketball. The third document does not. Accordingly, the first two documents are closer to each other than they are to the third document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO add two documents to the corpus. \n",
      "# One document should be more similar to the first document than the others, \n",
      "# and the other document should be more similar to the third document than the others.\n",
      "# TODO Measure their distances."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Document Classification\n",
      "\n",
      "Let's use our bag-of-words feature representations with a logistic regressor to classify documents. We will build a contemporary version of the canonical supervised machine learning application, the spam classifier. Instead of classifying email, we will classify SMS messages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loading and Inspecting the Data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Import numpy as np, pandas as pd, and pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# TODO read `sms.csv` into a `DataFrame`.\n",
      "df = pd.read_csv('sms.csv')\n",
      "# TODO what are the dataframe's columns?\n",
      "print df.columns.values\n",
      "# TODO are the classes balanced? Use the `value_counts()` method to count the number of spam and ham messages.\n",
      "print df['label'].value_counts()\n",
      "# TODO inspect the data. Print the first 10 spam and ham messages\n",
      "print df['message'][df['label'] == 1].head(10)\n",
      "print df['message'][df['label'] == 0].head(10)\n",
      "# TODO assign the 'label' column to y, and the 'message' column to X.\n",
      "y_raw = df['label']\n",
      "X_raw = df['message']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['label' 'message']\n",
        "0    4827\n",
        "1     747\n",
        "dtype: int64\n",
        "2     Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "5     FreeMsg Hey there darling it's been 3 week's n...\n",
        "8     WINNER!! As a valued network customer you have...\n",
        "9     Had your mobile 11 months or more? U R entitle...\n",
        "11    SIX chances to win CASH! From 100 to 20,000 po...\n",
        "12    URGENT! You have won a 1 week FREE membership ...\n",
        "15    XXXMobileMovieClub: To use your credit, click ...\n",
        "19    England v Macedonia - dont miss the goals/team...\n",
        "34    Thanks for your subscription to Ringtone UK yo...\n",
        "42    07732584351 - Rodger Burns - MSG = We tried to...\n",
        "Name: message, dtype: object\n",
        "0     Go until jurong point, crazy.. Available only ...\n",
        "1                         Ok lar... Joking wif u oni...\n",
        "3     U dun say so early hor... U c already then say...\n",
        "4     Nah I don't think he goes to usf, he lives aro...\n",
        "6     Even my brother is not like to speak with me. ...\n",
        "7     As per your request 'Melle Melle (Oru Minnamin...\n",
        "10    I'm gonna be home soon and i don't want to tal...\n",
        "13    I've been searching for the right words to tha...\n",
        "14                  I HAVE A DATE ON SUNDAY WITH WILL!!\n",
        "16                           Oh k...i'm watching here:)\n",
        "Name: message, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partitioning and Transforming the Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `train_test_split` convenience function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import train_test_split\n",
      "# TODO split the data set into training and testing sets\n",
      "# X_train, X_test, y_train, y_test = \n",
      "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw)\n",
      "\n",
      "# TODO fit the `binary_vectorizer` on the training set\n",
      "X_train = binary_vectorizer.fit_transform(X_train)\n",
      "# TODO how many words are in the vocabulary?\n",
      "print len(binary_vectorizer.vocabulary_)\n",
      "# TODO transform the training set\n",
      "# TODO transform the test set\n",
      "X_test = binary_vectorizer.transform(X_test)\n",
      "# TODO print X_train\n",
      "print X_train.todense()\n",
      "# TODO what are the dimensions of the design matrix?\n",
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7508\n",
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(4180, 7508)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the Model "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `LogisticRegression` class from the `linear_model` module.\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "# TODO create an instance of `LogisticRegression`\n",
      "# classifier = \n",
      "classifier = LogisticRegression()\n",
      "# TODO fit the classifier on the training data\n",
      "classifier.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating the Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO evaluate the classifier. What is the default performance measure for `LogisticRegression`? \n",
      "# (Hint: consult the documentation)\n",
      "print 'accuracy', classifier.score(X_test, y_test)\n",
      "# Let's evaluate the classifier using other performance measures.\n",
      "# TODO import the `recall_score`, `precision_score`, and `f1_score` functions from the `metrics` module.\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "# TODO make predictions for the test set\n",
      "# predictions = \n",
      "predictions = classifier.predict(X_test)\n",
      "# TODO evaluate the classifier's precision, recall, and F1 score.\n",
      "print 'recall', recall_score(y_test, predictions)\n",
      "print 'precision', precision_score(y_test, predictions)\n",
      "print 'f1', f1_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy 0.984218077475\n",
        "recall 0.883597883598\n",
        "precision 1.0\n",
        "f1 0.938202247191\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluating the classifier on a single training and test partition doesn't provide the best estimate of its performance. What if the class proportions are imbalanced, and all of the instances of one class appear only in one of the partitions?  \n",
      "Instead, we will evaluate the model using *cross validation*. We will make `n` partitions. We will train on `n-1` partitions and evaluate on the nth partition. We will then rotate the partitions, retrain, and re-test. \n",
      "We will continue rotating the partitions until we have trained and evaluated using all of the partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `cross_val_score` function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "# `cross_val_score()` takes the estimator object, the design matrix, and the values of the response variable.\n",
      "# It also takes the `cv` keyword argument to set the number of cross validation partitions.\n",
      "# `cross_val_score()` will train and test with each partition.\n",
      "# This means that we cannot fit the `CountVectorizer` once; we must fit it for each cross validation fold.\n",
      "# We can combine the `CountVectorizer` and the classifier to form a `Pipeline`. We can then pass the `Pipeline` to `cross_val_score()`.\n",
      "from sklearn.pipeline import Pipeline\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(binary=True)),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "print accuracy_scores, np.mean(accuracy_scores)\n",
      "# TODO now evaluate the classifier's `recall`, `precision`, and `F1` score using cross validation.\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print precision_scores, np.mean(precision_scores)\n",
      "print recall_scores, np.mean(recall_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.98476703  0.98566308  0.98114901  0.98294434  0.98294434] 0.9834935619\n",
        "[ 1.          1.          0.99230769  1.          0.99242424]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.996946386946\n",
        "[ 0.88666667  0.89333333  0.86577181  0.87248322  0.87919463] 0.879489932886\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn\n",
      "\n",
      "Find or create a set of documents for a binary classification problem. Produce feature representations for your documents using the bag-of-words model, and classify the documents using logistic regression."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}